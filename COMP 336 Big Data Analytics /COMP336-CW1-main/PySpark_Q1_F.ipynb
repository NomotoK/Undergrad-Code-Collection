{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "def convert_time_to_gmt8(input_df):\n",
    "\n",
    "\n",
    "    # 复制输入 DataFrame，以避免在原始 DataFrame 上进行修改\n",
    "    result_df = input_df\n",
    "\n",
    "    # 定义一个函数来将时间增加 8 小时\n",
    "    def add_eight_hours(time_str, date_str):\n",
    "        # 将时间和日期字符串组合成完整的日期时间字符串\n",
    "        datetime_str = F.concat_ws(\" \", date_str, time_str)\n",
    "        \n",
    "        # 将日期时间字符串解析为 Timestamp 对象\n",
    "        timestamp = F.unix_timestamp(datetime_str, \"yyyy-MM-dd HH:mm:ss\").cast(\"timestamp\")\n",
    "        \n",
    "        # 增加 8 小时\n",
    "        gmt8_time = timestamp + F.expr(\"INTERVAL 8 HOURS\")\n",
    "    \n",
    "        # 将 Timestamp 对象格式化为 HH:mm:ss 字符串和 YYYY-MM-DD 日期字符串\n",
    "        formatted_time = F.date_format(gmt8_time, \"HH:mm:ss\")\n",
    "        formatted_date = F.date_format(gmt8_time, \"yyyy-MM-dd\")\n",
    "\n",
    "        \n",
    "        return formatted_date, formatted_time, gmt8_time\n",
    "    \n",
    "\n",
    "\n",
    "    # 使用 withColumn 转换将 \"Date\" 列和 \"Time\" 列的时间值增加 8 小时\n",
    "    result_df = result_df.withColumn(\n",
    "        \"Date\",\n",
    "        add_eight_hours(col(\"Time\"), col(\"Date\"))[0]\n",
    "    ).withColumn(\n",
    "        \"Time\",\n",
    "        add_eight_hours(col(\"Time\"), col(\"Date\"))[1]\n",
    "    ).withColumn(\n",
    "        \"TimeDate\",\n",
    "        F.concat_ws(\" \", col(\"Date\"), col(\"Time\"))\n",
    "    ).withColumn(\n",
    "        \"Basedate\",\n",
    "        F.unix_timestamp(F.lit(\"1899-12-30 00:00:00\"), \"yyyy-MM-dd HH:mm:ss\").cast(\"timestamp\")\n",
    "    ).withColumn(    \n",
    "        \"Timestamp\",\n",
    "        ((F.unix_timestamp(col(\"TimeDate\"), \"yyyy-MM-dd HH:mm:ss\") - F.unix_timestamp(col(\"Basedate\"), \"yyyy-MM-dd HH:mm:ss\"))/ 86400)\n",
    "    )\n",
    "\n",
    "    result_df = result_df.drop(\"TimeDate\")\n",
    "    q1_df = result_df.drop(\"Basedate\")\n",
    "    return q1_df\n",
    "\n",
    "\n",
    "\n",
    "def top_five_users(input_df):\n",
    "    # Create a new column \"DateTimestamp\" by combining \"Date\" and \"Time\" columns\n",
    "    data_with_datetime = result_data.withColumn(\n",
    "        \"DateTimestamp\",\n",
    "        F.concat_ws(\" \", col(\"Date\"), col(\"Time\"))\n",
    "    )\n",
    "\n",
    "    # Convert \"DateTimestamp\" to timestamp type\n",
    "    data_with_datetime = data_with_datetime.withColumn(\n",
    "        \"DateTimestamp\",\n",
    "        F.unix_timestamp(col(\"DateTimestamp\"), \"yyyy-MM-dd HH:mm:ss\").cast(\"timestamp\")\n",
    "    )\n",
    "\n",
    "    # Create a window specification for each user and date\n",
    "    user_window_spec = Window.partitionBy(\"UserID\", F.to_date(\"DateTimestamp\")).orderBy(\"DateTimestamp\")\n",
    "\n",
    "    # Count the number of data points for each user on each day\n",
    "    data_with_count = data_with_datetime.withColumn(\n",
    "        \"DataPointsCount\",\n",
    "        F.count(\"UserID\").over(user_window_spec)\n",
    "    )\n",
    "\n",
    "    # Filter days with at least five data points\n",
    "    data_filtered = data_with_count.filter(col(\"DataPointsCount\") >= 5)\n",
    "\n",
    "    # Count the number of days with at least five data points for each user\n",
    "    user_days_count = data_filtered.groupBy(\"UserID\").agg(F.countDistinct(F.to_date(\"DateTimestamp\")).alias(\"DaysCount\"))\n",
    "\n",
    "    # Rank users based on the number of days with at least five data points\n",
    "    user_ranked = user_days_count.withColumn(\n",
    "        \"Rank\",\n",
    "        F.rank().over(Window.orderBy(col(\"DaysCount\").desc(), col(\"UserID\")))\n",
    "    )\n",
    "\n",
    "    # Filter the top 5 users\n",
    "    q2_df = user_ranked.filter(col(\"Rank\") <= 5).select(\"UserID\", \"DaysCount\")\n",
    "\n",
    "    return q2_df\n",
    "\n",
    "\n",
    "def calculate_weeks_with_data(input_df):\n",
    "    # Assuming the input DataFrame has the necessary columns and transformations\n",
    "    # If not, apply the necessary transformations to convert time and timestamp columns\n",
    "    \n",
    "    # Create a window specification for each user and week starting from Monday\n",
    "    user_week_window_spec = Window.partitionBy(\"UserID\", F.year(\"DateTimestamp\"), F.weekofyear(\"DateTimestamp\", startDayOfWeek=2)).orderBy(\"DateTimestamp\")\n",
    "\n",
    "    # Count the number of data points for each user in each week\n",
    "    data_with_count = input_df.withColumn(\n",
    "        \"DataPointsCount\",\n",
    "        F.count(\"UserID\").over(user_week_window_spec)\n",
    "    )\n",
    "\n",
    "    # Filter weeks with more than 100 data points\n",
    "    data_filtered = data_with_count.filter(col(\"DataPointsCount\") > 100)\n",
    "\n",
    "    # Count the number of weeks with more than 100 data points for each user\n",
    "    q3_df = data_filtered.groupBy(\"UserID\").agg(F.countDistinct(F.concat(F.year(\"DateTimestamp\"), F.weekofyear(\"DateTimestamp\", startDayOfWeek=2))).alias(\"WeeksCount\"))\n",
    "\n",
    "    # Show the result\n",
    "    return q3_df\n",
    "\n",
    "\n",
    "\n",
    "# 创建 Spark session\n",
    "spark = SparkSession.builder.appName(\"TimeConversion\").getOrCreate()\n",
    "\n",
    "# 请替换 \"your_data.csv\" 为实际数据文件路径\n",
    "file_path = \"/content/drive/MyDrive/Colab Notebooks/dataset.txt\"\n",
    "raw_data = spark.read.option(\"header\", \"true\").csv(file_path)\n",
    "\n",
    "# 使用函数进行转换\n",
    "q1 = convert_time_to_gmt8(raw_data)\n",
    "q1.show()\n",
    "\n",
    "q2 = top_five_users(q1)\n",
    "q2.show()\n",
    "\n",
    "q3 = calculate_weeks_with_data(q1)\n",
    "q3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1_F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "def convert_time_to_gmt8(input_df):\n",
    "    # 创建 Spark session\n",
    "    spark = SparkSession.builder.appName(\"TimeConversion\").getOrCreate()\n",
    "\n",
    "    # 复制输入 DataFrame，以避免在原始 DataFrame 上进行修改\n",
    "    result_df = input_df\n",
    "\n",
    "    # 定义一个函数来将时间增加 8 小时\n",
    "    def add_eight_hours(time_str, date_str):\n",
    "        # 将时间和日期字符串组合成完整的日期时间字符串\n",
    "        datetime_str = F.concat_ws(\" \", date_str, time_str)\n",
    "        \n",
    "        # 将日期时间字符串解析为 Timestamp 对象\n",
    "        timestamp = F.unix_timestamp(datetime_str, \"yyyy-MM-dd HH:mm:ss\").cast(\"timestamp\")\n",
    "        \n",
    "        # 增加 8 小时\n",
    "        gmt8_time = timestamp + F.expr(\"INTERVAL 8 HOURS\")\n",
    "\n",
    "        # 转换为从 1899 年 12 月 30 日的天数\n",
    "        # days_since_1899 = (F.datediff(gmt8_time, F.lit(\"1899-12-30\")))\n",
    "    \n",
    "        # 将 Timestamp 对象格式化为 HH:mm:ss 字符串和 YYYY-MM-DD 日期字符串\n",
    "        formatted_time = F.date_format(gmt8_time, \"HH:mm:ss\")\n",
    "        formatted_date = F.date_format(gmt8_time, \"yyyy-MM-dd\")\n",
    "\n",
    "        # convert\"1899-12-30 00:00:00\" to timestamp\n",
    "        # base_time = F.unix_timestamp(F.lit(\"1899-12-30 00:00:00\"), \"yyyy-MM-dd HH:mm:ss\").cast(\"timestamp\")\n",
    "        \n",
    "        return formatted_date, formatted_time, gmt8_time\n",
    "    \n",
    "\n",
    "\n",
    "    # 使用 withColumn 转换将 \"Date\" 列和 \"Time\" 列的时间值增加 8 小时\n",
    "    result_df = result_df.withColumn(\n",
    "        \"Date\",\n",
    "        add_eight_hours(col(\"Time\"), col(\"Date\"))[0]\n",
    "    ).withColumn(\n",
    "        \"Time\",\n",
    "        add_eight_hours(col(\"Time\"), col(\"Date\"))[1]\n",
    "    ).withColumn(\n",
    "        \"TimeDate\",\n",
    "        F.concat_ws(\" \", col(\"Date\"), col(\"Time\"))\n",
    "    ).withColumn(\n",
    "        \"Basedate\",\n",
    "        F.unix_timestamp(F.lit(\"1899-12-30 00:00:00\"), \"yyyy-MM-dd HH:mm:ss\").cast(\"timestamp\")\n",
    "    ).withColumn(    \n",
    "        \"Timestamp\",\n",
    "        ((F.unix_timestamp(col(\"TimeDate\"), \"yyyy-MM-dd HH:mm:ss\") - F.unix_timestamp(col(\"Basedate\"), \"yyyy-MM-dd HH:mm:ss\"))/ 86400)\n",
    "    )\n",
    "\n",
    "    result_df = result_df.drop(\"TimeDate\")\n",
    "    result_df = result_df.drop(\"Basedate\")\n",
    "    return result_df\n",
    "\n",
    "\n",
    "# 示例用法\n",
    "# 请替换 \"your_data.csv\" 为实际数据文件路径\n",
    "file_path = \"/content/drive/MyDrive/Colab Notebooks/dataset.txt\"\n",
    "raw_data = spark.read.option(\"header\", \"true\").csv(file_path)\n",
    "\n",
    "# 使用函数进行转换\n",
    "result_data = convert_time_to_gmt8(raw_data)\n",
    "\n",
    "# 显示结果\n",
    "result_data.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_five_users(input_df):\n",
    "    # Create a new column \"DateTimestamp\" by combining \"Date\" and \"Time\" columns\n",
    "    data_with_datetime = result_data.withColumn(\n",
    "        \"DateTimestamp\",\n",
    "        F.concat_ws(\" \", col(\"Date\"), col(\"Time\"))\n",
    "    )\n",
    "\n",
    "    # Convert \"DateTimestamp\" to timestamp type\n",
    "    data_with_datetime = data_with_datetime.withColumn(\n",
    "        \"DateTimestamp\",\n",
    "        F.unix_timestamp(col(\"DateTimestamp\"), \"yyyy-MM-dd HH:mm:ss\").cast(\"timestamp\")\n",
    "    )\n",
    "\n",
    "    # Create a window specification for each user and date\n",
    "    user_window_spec = Window.partitionBy(\"UserID\", F.to_date(\"DateTimestamp\")).orderBy(\"DateTimestamp\")\n",
    "\n",
    "    # Count the number of data points for each user on each day\n",
    "    data_with_count = data_with_datetime.withColumn(\n",
    "        \"DataPointsCount\",\n",
    "        F.count(\"UserID\").over(user_window_spec)\n",
    "    )\n",
    "\n",
    "    # Filter days with at least five data points\n",
    "    data_filtered = data_with_count.filter(col(\"DataPointsCount\") >= 5)\n",
    "\n",
    "    # Count the number of days with at least five data points for each user\n",
    "    user_days_count = data_filtered.groupBy(\"UserID\").agg(F.countDistinct(F.to_date(\"DateTimestamp\")).alias(\"DaysCount\"))\n",
    "\n",
    "    # Rank users based on the number of days with at least five data points\n",
    "    user_ranked = user_days_count.withColumn(\n",
    "        \"Rank\",\n",
    "        F.rank().over(Window.orderBy(col(\"DaysCount\").desc(), col(\"UserID\")))\n",
    "    )\n",
    "\n",
    "    # Filter the top 5 users\n",
    "    top_users = user_ranked.filter(col(\"Rank\") <= 5).select(\"UserID\", \"DaysCount\")\n",
    "\n",
    "    return top_users\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "def calculate_weeks_with_data(input_df):\n",
    "    # Assuming the input DataFrame has the necessary columns and transformations\n",
    "    # If not, apply the necessary transformations to convert time and timestamp columns\n",
    "    \n",
    "    # Create a window specification for each user and week\n",
    "    user_week_window_spec = Window.partitionBy(\"UserID\", F.year(\"DateTimestamp\"), F.weekofyear(\"DateTimestamp\")).orderBy(\"DateTimestamp\")\n",
    "\n",
    "    # Count the number of data points for each user in each week\n",
    "    data_with_count = input_df.withColumn(\n",
    "        \"DataPointsCount\",\n",
    "        F.count(\"UserID\").over(user_week_window_spec)\n",
    "    )\n",
    "\n",
    "    # Filter weeks with more than 100 data points\n",
    "    data_filtered = data_with_count.filter(col(\"DataPointsCount\") > 100)\n",
    "\n",
    "    # Count the number of weeks with more than 100 data points for each user\n",
    "    user_weeks_count = data_filtered.groupBy(\"UserID\").agg(F.countDistinct(F.concat(F.year(\"DateTimestamp\"), F.weekofyear(\"DateTimestamp\"))).alias(\"WeeksCount\"))\n",
    "\n",
    "    # Show the result\n",
    "    user_weeks_count.show()\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"UserDataAnalysis\").getOrCreate()\n",
    "\n",
    "# Read the dataset\n",
    "file_path = \"your_data.csv\"  # Replace with the actual path to your data file\n",
    "raw_data = spark.read.option(\"header\", \"true\").csv(file_path)\n",
    "\n",
    "# Assuming the dataset has been processed using the previous conversion function\n",
    "# If not, apply the necessary transformations to convert time and timestamp columns\n",
    "\n",
    "# Create a new column \"DateTimestamp\" by combining \"Date\" and \"Time\" columns\n",
    "data_with_datetime = raw_data.withColumn(\n",
    "    \"DateTimestamp\",\n",
    "    F.concat_ws(\" \", col(\"Date\"), col(\"Time\"))\n",
    ")\n",
    "\n",
    "# Convert \"DateTimestamp\" to timestamp type\n",
    "data_with_datetime = data_with_datetime.withColumn(\n",
    "    \"DateTimestamp\",\n",
    "    F.unix_timestamp(col(\"DateTimestamp\"), \"yyyy-MM-dd HH:mm:ss\").cast(\"timestamp\")\n",
    ")\n",
    "\n",
    "# Call the function to calculate weeks with more than 100 data points\n",
    "calculate_weeks_with_data(data_with_datetime)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
